{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f560c708",
   "metadata": {},
   "source": [
    "# Chapter 8 recap - Training a model for movie review classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588eb7f",
   "metadata": {},
   "source": [
    "This section is a recap of the logistic regression model that was trained in the last section of Chapter 6. Execute the following code blocks to train a model that we will serialize in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19888917",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "The code below is based on the `movie_data.csv` dataset that was created in Chapter 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48159df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sann-\n",
      "[nltk_data]     htet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d368b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedec428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed8efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c69dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cea6d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:39\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f61897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96512d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585dcea",
   "metadata": {},
   "source": [
    "# Serializing fitted scikit-learn estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6c5db6",
   "metadata": {},
   "source": [
    "After we trained the logistic regression model as shown above, we now save the classifier along woth the stop words, Porter Stemmer, and `HashingVectorizer` as serialized objects to our local disk so that we can use the fitted classifier in our web application later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ee26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)\n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ff53cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting movieclassifier/vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile movieclassifier/vectorizer.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(os.path.join(cur_dir, 'pkl_objects', 'stopwords.pkl'), 'rb'))\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "                   + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc750c0",
   "metadata": {},
   "source": [
    "After executing the preceeding code cells, we can now restart the IPython notebook kernel to check if the objects were serialized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4066764",
   "metadata": {},
   "source": [
    "First, change the current Python directory to `movieclassifer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5166bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('movieclassifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8bf1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44bd2e",
   "metadata": {},
   "source": [
    "After we have successfully loaded the `vectorizer` and unpickled the classifier, we\n",
    "can use these objects to preprocess document examples and make predictions about\n",
    "their sentiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e24449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 95.55%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "example = [\"I love this movie. It's amazing\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' % (label[clf.predict(X)[0]],\n",
    "                                               np.max(clf.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652dbae",
   "metadata": {},
   "source": [
    "# Setting up a SQLite database for data storage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246c041",
   "metadata": {},
   "source": [
    "Note that we are still on the \"movieclassifier\" subdirectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef7ae861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sann-htet/Desktop/Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow/Chapter 9/movieclassifier'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4618a441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "c.execute('DROP TABLE IF EXISTS review_db')\n",
    "c.execute('CREATE TABLE review_db(review TEXT, sentiment INTEGER, date TEXT)')\n",
    "\n",
    "example1 = 'I love this movie'\n",
    "c.execute(\"INSERT INTO review_db(review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example1, 1))\n",
    "\n",
    "example2 = 'I disliked this movie'\n",
    "c.execute(\"INSERT INTO review_db(review, sentiment, date) VALUES (?, ?, DATETIME('now'))\", (example2, 0))\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb1e50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute(\"SELECT * FROM review_db WHERE date BETWEEN '2023-01-01 00:00:00' AND DATETIME('now')\")\n",
    "results = c.fetchall()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "befe56c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love this movie', 1, '2023-07-31 11:23:27'), ('I disliked this movie', 0, '2023-07-31 11:23:27')]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398fe4f",
   "metadata": {},
   "source": [
    "# Developing a web application with Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd01f75",
   "metadata": {},
   "source": [
    "## Our first Flask web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "104ebd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef59bd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1st_flask_app_1',\n",
       " 'movie_data.csv',\n",
       " '.ipynb_checkpoints',\n",
       " 'movieclassifier',\n",
       " 'ch09-Embedding_a_Machine_Learning_Model_into_a_Web_Application.ipynb']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17da28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('1st_flask_app_1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae15e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['templates', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "558f116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('first_app.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3358f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('templates/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8209908c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing first_app.html\n"
     ]
    }
   ],
   "source": [
    "%%writefile first_app.html\n",
    "<!doctype html>\n",
    "<html>\n",
    "    <head>\n",
    "        <title>First app</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>Hi, this is my first Flask web app!</div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01c66fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c043654",
   "metadata": {},
   "source": [
    "Now, let's start our web application by executing the command from the\n",
    "terminal inside the `1st_flask_app_1` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51ef3562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/sann-htet/anaconda3/envs/tensorflow_gpu/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      " * Serving Flask app 'app'\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [31/Jul/2023 18:35:32] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [31/Jul/2023 18:35:32] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712e78ee",
   "metadata": {},
   "source": [
    "## Form validation and rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc9a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
